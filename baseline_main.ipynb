{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline_main.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNjzCOuVKzAcdBcIY18uZx6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLDL-FederatedLearning-project/FederatedLearning/blob/main/baseline_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "ePhuJffQATzh",
        "outputId": "bf9aad73-4a13-4514-ddf4-649a41b01761"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "from options import args_parser\n",
        "from update import test_inference, validation\n",
        "from models import CNN\n",
        "from dataset_split import get_train_valid_loader, get_test_loader\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d6fd29249139>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margs_parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest_inference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'options'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmGGFDtpAwig"
      },
      "source": [
        "\n",
        "args = args_parser()\n",
        "if args.gpu:\n",
        "    torch.cuda.set_device(args.gpu)\n",
        "device = 'cuda' if args.gpu else 'cpu'\n",
        "\n",
        "SEED = args.seed\n",
        "BATCH_SIZE = args.BATCH_SIZE\n",
        "DATA_DIR = args.DATA_DIR\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeRVTOd1BAIc"
      },
      "source": [
        "# load datasets\n",
        "trainloader, validloader = get_train_valid_loader(args,\n",
        "                                                      valid_size=0.2,\n",
        "                                                      shuffle=True,\n",
        "                                                      pin_memory=False)\n",
        "testloader = get_test_loader(args,\n",
        "                                 shuffle=True,\n",
        "                                 pin_memory=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbcXHRtmBDSs"
      },
      "source": [
        "# BUILD MODEL\n",
        "args.model = 'cnn'\n",
        "# Convolutional neural network\n",
        "args.dataset = 'cifar'\n",
        "global_model = CNN(args=args)\n",
        "\n",
        "# Set the model to train and send it to device.\n",
        "global_model.to(device)\n",
        "global_model.train()\n",
        "print(global_model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1az3a00CVPJ"
      },
      "source": [
        "# Training\n",
        "# Set optimizer and criterion\n",
        "args.optimizer = 'sgd'\n",
        "optimizer = torch.optim.SGD(global_model.parameters(), lr=args.lr,\n",
        "                            momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "criterion = torch.nn.NLLLoss().to(device)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "accuracy_train = 0\n",
        "accuracy_valid = 0\n",
        "accuracy_test=0\n",
        "\n",
        "epoch_accuracy_train = []\n",
        "epoch_accuracy_valid = []\n",
        "epoch_accuracy_test=[]\n",
        "\n",
        "loss_valid = 0\n",
        "\n",
        "epoch_loss_train = []\n",
        "epoch_loss_valid = []\n",
        "epoch_loss_test=[]\n",
        "\n",
        "for epoch in range(args.epochs):\n",
        "    batch_loss_train = []\n",
        "    batch_acc_train = []\n",
        "    for batch_idx, (images, labels) in enumerate(trainloader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = global_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, pred_labels = torch.max(outputs, 1)\n",
        "        pred_labels = pred_labels.view(-1)\n",
        "        correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
        "        total += len(labels)\n",
        "        accuracy_train = 100 * correct / total\n",
        "\n",
        "        batch_loss_train.append(loss.item())\n",
        "        batch_acc_train.append(accuracy_train)\n",
        "\n",
        "    loss_avg_train = sum(batch_loss_train) / len(batch_loss_train)\n",
        "    acc_avg_train = sum(batch_acc_train) / len(batch_acc_train)\n",
        "\n",
        "    epoch_loss_train.append(loss_avg_train)\n",
        "    epoch_accuracy_train.append(acc_avg_train)\n",
        "\n",
        "    print('Epoch:', epoch+1)\n",
        "    print('Train accuracy : {:.2f}%'.format(acc_avg_train), 'Train loss : {:.4f}'.format(loss_avg_train))\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    accuracy_valid, loss_valid = validation(args, global_model, validloader)\n",
        "    epoch_loss_valid.append(loss_valid)\n",
        "    epoch_accuracy_valid.append(accuracy_valid)\n",
        "    print('Valid accuracy : {:.2f}%'.format(accuracy_valid), 'Valid loss : {:.4f}'.format(loss_valid))\n",
        "\n",
        "    test_acc, test_loss = test_inference(args, global_model, testloader)\n",
        "    epoch_loss_test.append(test_loss)\n",
        "    epoch_accuracy_test.append(test_acc)\n",
        "    print('Test accuracy : {:.2f}%'.format(test_acc), 'Test loss : {:.4f}'.format(test_loss))\n",
        "\n",
        "\n",
        "#Centralized model graphs that we need for REPORT\n",
        "fig, (ax1, ax2) = plt.subplots(2)\n",
        "\n",
        "train_loss, = ax1.plot(range(len(epoch_loss_train)), epoch_loss_train)\n",
        "valid_loss, = ax1.plot(range(len(epoch_loss_valid)), epoch_loss_valid)\n",
        "loss_test, = ax1.plot(range(len(epoch_loss_test)), epoch_loss_test)\n",
        "ax1.legend([train_loss, valid_loss,loss_test], ['train loss', 'valid loss','test loss'])\n",
        "ax1.set_xlabel(\"Epoch\")\n",
        "ax1.set_ylabel(\"Loss\")\n",
        "train_acc, = ax2.plot(range(len(epoch_accuracy_train)), epoch_accuracy_train)\n",
        "valid_acc, = ax2.plot(range(len(epoch_accuracy_valid)), epoch_accuracy_valid)\n",
        "acc_test, = ax2.plot(range(len(epoch_accuracy_test)), epoch_accuracy_test)\n",
        "ax2.legend([train_acc, valid_acc, acc_test], ['train accuracy', 'valid accuracy', \"test accuracy\"])\n",
        "ax2.set_xlabel(\"Epoch\")\n",
        "ax2.set_ylabel(\"Accuracy\")\n",
        "\n",
        "\n",
        "plt.savefig('.../imagesReport/accuracy_loss_baseline.png'.format(args.dataset,args.model,args.epochs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isDyeFToDuRi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}